{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create your Index\n",
    "\n",
    "This notebook runs through the creation of an Azure AI Search index using locally stored documents. It uses Beautiful Soup to parse HTML documents, and then LangChain's UnstructuredHTMLLoader and CharacterTextSplitter to chunk the data. The sizes of these chunks can be configured.\n",
    "\n",
    "Vector embeddings are created using text-embedding-ada-002. Currently only the content chunks are embedded, but more can be added, such as title, if desired (see the commented fields in the index definition for examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "\n",
    "import openai\n",
    "from openai import AzureOpenAI, BadRequestError\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, Document, HTMLHeaderTextSplitter, TokenTextSplitter\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import UnstructuredHTMLLoader, BSHTMLLoader\n",
    "from langchain.document_transformers import BeautifulSoupTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    CorsOptions,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    SearchIndex,\n",
    "    SemanticConfiguration,\n",
    "    PrioritizedFields,\n",
    "    SemanticField,\n",
    "    SearchField,\n",
    "    SemanticSettings,\n",
    "    VectorSearch,\n",
    "    VectorSearchProfile,\n",
    "    HnswVectorSearchAlgorithmConfiguration,\n",
    "    HnswParameters,\n",
    ")\n",
    "\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")  # for exponential backoff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to set up a .env file first (see .env.template for an example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # take environment variables from .env."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a document out of the page, removing all of the table data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split a document into sections by h2 header.\n",
    "\n",
    "Within each section, check if there are tables. If there are tables, remove all attributes from the HTML tags, but leave the table tags intact - to improve the LLMs ability to parse the structure of the table.\n",
    "\n",
    "For the parts of the document that are not tables - process as normal, chunking as normal with overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse all the tables into Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(\n",
    "    stop=stop_after_attempt(5),\n",
    "    wait=wait_random_exponential(multiplier=1, max=10)\n",
    ")\n",
    "def generate_response(client, content, prompt, deployment_name):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "        {\"role\": \"user\", \"content\": content}\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=deployment_name,\n",
    "            messages=messages,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "    except BadRequestError as err:\n",
    "        print(err.message)\n",
    "        print(content)\n",
    "        return None\n",
    "        \n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARY_PROMPT = \"Summarize the given text below into 4 sentences or fewer. Use Bullet points to summarize multiple data points efficiently. If the data is in the form of an HTML table, include as much detailed information from the table as possible. \\n\\nText:\"\n",
    "\n",
    "TABLE_EXTRACTION_PROMPT = \"Given the below table, extract all of the relevant information contained within, and reformat it into a bulleted list. Each row should be one line, under 50 words apiece. Be as concise as possible while including all relevant figures, prices, and limitations of information in the row. \\n\\nTable:\"\n",
    "\n",
    "\n",
    "CLIENT = AzureOpenAI(\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\"),\n",
    "    api_version = \"2023-12-01-preview\",\n",
    "    azure_endpoint = os.getenv(\"OPENAI_ENDPOINT\")\n",
    ")\n",
    "DEPLOYMENT_NAME = 'gpt-35-turbo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_html_chunker(filepath):\n",
    "    table_docs = []\n",
    "    with open(filepath, \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "    if soup.title is not None:\n",
    "        title = soup.title.string\n",
    "    else:\n",
    "        title = filepath.split(\"/\")[-1]\n",
    "    if soup.find(\"meta\") is not None:\n",
    "        metatags = soup.find_all('meta')\n",
    "        for tag in metatags:\n",
    "            if tag.get('name', None) == 'pagePath':\n",
    "                source = tag['content']\n",
    "\n",
    "            if tag.get('name', None) == 'updateDate':\n",
    "                last_updated = tag['content']\n",
    "    else:\n",
    "        source = filepath\n",
    "        last_updated = None\n",
    "    tbls = soup.find_all(\"table\")\n",
    "\n",
    "    if len(soup.select(\"#universal-content\")) > 0:\n",
    "        sections = soup.select(\"#universal-content\")[0].prettify().split(\"<h2\")\n",
    "    else:\n",
    "        sections = soup.prettify().split(\"<h2\")\n",
    "\n",
    "    # Split doc by H2, look for tables in each section, create docs of each table\n",
    "    for section in sections:\n",
    "        section = \"<h2\" + section\n",
    "        section_soup = BeautifulSoup(section, \"html.parser\", from_encoding='utf-8')\n",
    "        tables = section_soup.find_all(\"table\")\n",
    "        if len(tables) > 0:\n",
    "            for table in tables:\n",
    "                del table.attrs\n",
    "                if table.find_previous('h3'):\n",
    "                    header = table.find_previous('h3').text.strip()\n",
    "                    if table.find_previous('h2'):\n",
    "                        header2 = table.find_previous('h2').text.strip()\n",
    "                        metadata = {\"title\": title, \"source\": source, \"Header 3\": header, \"Header 2\": header2}\n",
    "                    metadata = {\"title\": title, \"source\": source, \"Header 3\": header}\n",
    "                else:\n",
    "                    if table.find_previous('h2'):\n",
    "                        header = table.find_previous('h2').text\n",
    "                        metadata = {\"title\": title, \"source\": source, \"Header 2\": header.strip()}\n",
    "                    else:\n",
    "                        metadata = {\"title\": title, \"source\": source}\n",
    "                for tag in table.recursiveChildGenerator():\n",
    "                    if hasattr(tag, 'attrs'):\n",
    "                        tag.attrs = {}\n",
    "                page_content = table.prettify().replace(\"<p>\", \"\").replace(\"</p>\", \"\").replace(\"<br/>\", \"\").replace(\"<br>\", \"\")\n",
    "                \n",
    "                summary = generate_response(CLIENT, page_content[:10000], TABLE_EXTRACTION_PROMPT, DEPLOYMENT_NAME)\n",
    "                if summary is not None: \n",
    "                    metadata[\"summary\"] = summary\n",
    "                else:\n",
    "                    metadata[\"summary\"] = \"No summary generated\"\n",
    "                \n",
    "                table_doc = Document(page_content= page_content, metadata=metadata)\n",
    "                table_docs.append(table_doc)\n",
    "\n",
    "\n",
    "\n",
    "    tbls = soup.find_all(\"table\")\n",
    "    ## Remove tables from soup\n",
    "    for tbl in tbls:\n",
    "        tbl.decompose()\n",
    "\n",
    "    # Chunk the table-less content\n",
    "\n",
    "    headers_to_split_on = [\n",
    "        (\"h1\", \"Header 1\"),\n",
    "        (\"h2\", \"Header 2\"),\n",
    "        (\"h3\", \"Header 3\")\n",
    "    ]\n",
    "\n",
    "    html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)\n",
    "    html_header_splits = html_splitter.split_text(soup.prettify())\n",
    "\n",
    "    # Sometimes the header splits results in too large of documents for summarizing\n",
    "    length_checked_docs = []\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=4000, chunk_overlap=400)\n",
    "    for doc in html_header_splits:\n",
    "        if(len(doc.page_content)/4) > 5000:\n",
    "            split_docs = text_splitter.split_documents([doc])\n",
    "            for split_doc in split_docs:\n",
    "                length_checked_docs.append(split_doc)\n",
    "        else:\n",
    "            length_checked_docs.append(doc)\n",
    "\n",
    "    for split in length_checked_docs:\n",
    "        split.metadata[\"source\"] = source\n",
    "        split.metadata[\"title\"] = title\n",
    "        split.metadata[\"last_updated\"] = last_updated\n",
    "        split.metadata[\"summary\"] = generate_response(CLIENT, split.page_content, SUMMARY_PROMPT, DEPLOYMENT_NAME)\n",
    "\n",
    "    return length_checked_docs + table_docs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use BeautifulSoup to parse HTML\n",
    "This section assumes your data is stored locally for the time being - this will need to be adjusted for scale.\n",
    "Set your root_dir to where you have saved your documents to be indexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "root_dir = \"./data/\"\n",
    "\n",
    "# Loop through the folders\n",
    "docs = []\n",
    "failed = []\n",
    "cnt = 0\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    for file in tqdm(filenames):\n",
    "        #try:\n",
    "        if file.endswith('.pdf'):\n",
    "            loader = PyPDFLoader(os.path.join(dirpath, file))\n",
    "            docs.extend(loader.load_and_split())\n",
    "        elif file.endswith('.docx'):\n",
    "            loader = Docx2txtLoader(os.path.join(dirpath, file))\n",
    "            docs.extend(loader.load_and_split())\n",
    "        elif file.endswith('.txt'):\n",
    "            loader = TextLoader(os.path.join(dirpath, file), encoding=\"utf-8\")\n",
    "            docs.extend(loader.load_and_split()) \n",
    "        elif file.endswith('.html'):\n",
    "            try:\n",
    "                docs += custom_html_chunker(os.path.join(dirpath, file))\n",
    "            except Exception as e:\n",
    "                print(file)\n",
    "                print(e)\n",
    "                failed.append(os.path.join(dirpath, file))\n",
    "            #print(\"File #\" + str(cnt) + \" \" + os.path.join(dirpath, file))\n",
    "            cnt = cnt + 1\n",
    "\n",
    "        else:\n",
    "        # Handle other file types\n",
    "            pass\n",
    "        # except Exception as e:\n",
    "        #     print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Search Index\n",
    "\n",
    "This method creates an Azure AI Search Index using the azure.search.documents SDK. These fields are configurable, but you will need to make sure they are then updated in the `QnA Promptflow` `flow.dag.yaml` file to match, along with any of the `.py` files that rely on those fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Search Index\n",
    "def create_acs_index(\n",
    "    service_endpoint, index_name, key\n",
    "):\n",
    "    credential = AzureKeyCredential(key)\n",
    "\n",
    "    # Create a search index\n",
    "    index_client = SearchIndexClient(endpoint=service_endpoint, credential=credential)\n",
    "    fields = [\n",
    "        SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "        SearchableField(\n",
    "            name=\"page_content\",\n",
    "            type=SearchFieldDataType.String,\n",
    "            searchable=True,\n",
    "            retrievable=True,\n",
    "        ),\n",
    "        SearchableField(\n",
    "            name=\"title\",\n",
    "            type=SearchFieldDataType.String,\n",
    "            searchable=True,\n",
    "            retrievable=True,\n",
    "        ),\n",
    "        SearchableField(\n",
    "            name=\"headers\",\n",
    "            type=SearchFieldDataType.String,\n",
    "            searchable=True,\n",
    "            retrievable=True,\n",
    "        ),\n",
    "        SearchableField(\n",
    "            name=\"summary\",\n",
    "            type=SearchFieldDataType.String,\n",
    "            searchable=True,\n",
    "            retrievable=True,\n",
    "        ),\n",
    "        # SearchableField(\n",
    "        #     name=\"url\",\n",
    "        #     type=SearchFieldDataType.String,\n",
    "        #     filterable=True,\n",
    "        #     searchable=False,\n",
    "        #     retrievable=True,\n",
    "        # ),\n",
    "        SearchableField(\n",
    "            name=\"last_updated\",\n",
    "            type=SearchFieldDataType.DateTimeOffset,\n",
    "            retrievable=True,\n",
    "        ),\n",
    "        SearchableField(\n",
    "            name=\"source\",\n",
    "            type=SearchFieldDataType.String,\n",
    "            filterable=True,\n",
    "            searchable=False,\n",
    "            retrievable=True,\n",
    "        ),\n",
    "        SearchField(\n",
    "            name=\"contentVector\",\n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "            searchable=True,\n",
    "            vector_search_dimensions=int(1536),\n",
    "            vector_search_profile=\"my-vector-search-profile\",\n",
    "        ),\n",
    "        # These are example additional vector fields or string fields that can be added to the index\n",
    "        # SearchField(\n",
    "        #     name=\"contentTitle\",\n",
    "        #     type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "        #     searchable=True,\n",
    "        #     vector_search_dimensions=int(dimension),\n",
    "        #     vector_search_profile=\"my-vector-search-profile\",\n",
    "        # ),\n",
    "        SearchField(\n",
    "            name=\"contentSummary\",\n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "            searchable=True,\n",
    "            vector_search_dimensions=int(1536),\n",
    "            vector_search_profile=\"my-vector-search-profile\",\n",
    "        ),\n",
    "        # SearchField(\n",
    "        #     name=\"contentDescription\",\n",
    "        #     type=SearchFieldDataType.String,\n",
    "        #     sortable=True,\n",
    "        #     filterable=True,\n",
    "        #     facetable=True,\n",
    "        #     analyzer_name=analyzer,\n",
    "        # ),\n",
    "    ]\n",
    "\n",
    "    # This configures the vector search algorithm. The parameters are set to the defaults for the algorithm.\n",
    "    vector_search = VectorSearch(\n",
    "        algorithms=[\n",
    "            HnswVectorSearchAlgorithmConfiguration(\n",
    "                name=\"my-vector-config\",\n",
    "                parameters=HnswParameters(\n",
    "                    m=4,\n",
    "                    ef_construction=int(400),\n",
    "                    ef_search=int(400),\n",
    "                    metric=\"cosine\",\n",
    "                ),\n",
    "            )\n",
    "        ],\n",
    "        profiles=[\n",
    "            VectorSearchProfile(\n",
    "                name=\"my-vector-search-profile\", algorithm=\"my-vector-config\"\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    semantic_config = SemanticConfiguration(\n",
    "        name=\"vzw-semantic-config\",\n",
    "        prioritized_fields=PrioritizedFields(\n",
    "            prioritized_content_fields=[\n",
    "                SemanticField(field_name=\"page_content\"),\n",
    "                SemanticField(field_name=\"summary\")\n",
    "            ],\n",
    "            title_field=SemanticField(field_name=\"title\"),            \n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Create the semantic settings with the configuration\n",
    "    semantic_settings = SemanticSettings(configurations=[semantic_config])\n",
    "\n",
    "\n",
    "    cors_options = CorsOptions(allowed_origins=[\"*\"], max_age_in_seconds=60)\n",
    "    scoring_profiles = []\n",
    "\n",
    "    # Create the search index with the semantic, tokenizer, and filter settings\n",
    "    index = SearchIndex(\n",
    "        name=index_name,\n",
    "        fields=fields,\n",
    "        vector_search=vector_search,\n",
    "        semantic_settings=semantic_settings,\n",
    "        scoring_profiles=scoring_profiles,\n",
    "        cors_options=cors_options,\n",
    "    )\n",
    "    result = index_client.create_or_update_index(index)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set your index name, credentials, and create the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "search_key = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "endpoint=os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")\n",
    "\n",
    "index_name = \"vbg-index-summaries-v2\"\n",
    "\n",
    "create_acs_index(service_endpoint=endpoint, index_name=index_name, key=search_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method connects to the Azure OpenAI service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@retry(wait=wait_random_exponential(min=5, max=90), stop=stop_after_attempt(6))\n",
    "def get_embedding(client, content):\n",
    "    return client.embeddings.create(model=os.environ.get('OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME'), input=content).data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the embeddings\n",
    "This took about 5 minute for the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),  \n",
    "    api_version=os.getenv('OPENAI_API_VERSION'),\n",
    "    azure_endpoint =os.getenv('OPENAI_ENDPOINT'),\n",
    "    )\n",
    "# Add embeddings\n",
    "for doc in tqdm(docs):\n",
    "    if len(doc.page_content) < 22000:\n",
    "        doc.metadata[\"embedding\"] = get_embedding(client, doc.page_content)\n",
    "    else:\n",
    "        doc.metadata[\"embedding\"] = get_embedding(client, doc.page_content[:22000])\n",
    "    if doc.metadata[\"summary\"] is not None:\n",
    "        doc.metadata[\"summary_embedding\"] = get_embedding(client, doc.metadata[\"summary\"])\n",
    "    else:\n",
    "        doc.metadata[\"summary_embedding\"] = get_embedding(client, \"No summary generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload to Azure AI Search\n",
    "\n",
    "This takes about 3-5 minutes for the test dataset, very dependent on networking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "props = ['Header 1', 'Header 2', 'Header 3', 'source', 'title', 'last_updated', 'summary', 'embedding', 'summary_embedding']\n",
    "for doc in docs:\n",
    "    for key in props:\n",
    "        if key in doc.metadata:\n",
    "            if doc.metadata[key] is None:\n",
    "                if key == \"summary_embedding\":\n",
    "                    doc.metadata[key] = get_embedding(client, \"No summary generated\")\n",
    "                else:\n",
    "                    doc.metadata[key] = \"\"\n",
    "        else:\n",
    "            doc.metadata[key] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs[5].metadata['last_updated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "credential = AzureKeyCredential(search_key)\n",
    "\n",
    "header_options = [\n",
    "    \"Header 1\",\n",
    "    \"Header 2\",\n",
    "    \"Header 3\"\n",
    "]\n",
    "\n",
    "search_client = SearchClient(\n",
    "    endpoint=endpoint, index_name=index_name, credential=credential\n",
    ")\n",
    "documents = []\n",
    "for i, chunk in enumerate(tqdm(docs)):\n",
    "    # summary = generate_summary(str(chunk[\"content\"]), chat_model_name, temperature)\n",
    "    id = hashlib.md5(chunk.page_content.encode()).hexdigest()\n",
    "    headers = []\n",
    "    for header in header_options:\n",
    "        if header in chunk.metadata:\n",
    "            headers.append(chunk.metadata[header])\n",
    "\n",
    "    input_data = {\n",
    "        \"id\": str(id),\n",
    "        \"title\": str(chunk.metadata[\"title\"]),\n",
    "        \"page_content\": str(chunk.page_content),\n",
    "        \"source\": str(chunk.metadata[\"source\"]),\n",
    "        \"last_updated\": str(chunk.metadata[\"last_updated\"]),\n",
    "        \"contentVector\": chunk.metadata[\"embedding\"],\n",
    "        \"contentSummary\": chunk.metadata[\"summary_embedding\"],\n",
    "        \"summary\": str(chunk.metadata[\"summary\"]),\n",
    "        \"headers\": str(headers),\n",
    "    }\n",
    "\n",
    "    #print(input_data)\n",
    "\n",
    "    documents.append(input_data)\n",
    "    search_client.upload_documents(documents=[input_data])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
